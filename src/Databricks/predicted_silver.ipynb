{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ecf586b-d682-42a1-897f-70efc5345130",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.ops import unary_union\n",
    "from shapely.geometry import MultiPolygon\n",
    "from shapely.geometry.base import BaseGeometry\n",
    "from shapely import force_2d\n",
    "from shapely import wkt\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import DoubleType\n",
    "from delta.tables import DeltaTable\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9c07d4b-3b49-4d39-9044-495fc60de30a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predicted_masks = \"/Volumes/land_topografisk-gdb_dev/external_dev/static_data/DL_SNUPLASSER/predicted_masks\"\n",
    "catalog_dev = \"`land_topografisk-gdb_dev`\"\n",
    "schema_dev = \"ai2025\"\n",
    "log_table = f\"{catalog_dev}.{schema_dev}.logs_predicted_masks\"\n",
    "table = f\"{catalog_dev}.{schema_dev}.predicted_silver\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b983e36-e54a-4007-8da3-a2e5f1af97a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sett kontekst med katalog og skjema \n",
    "spark.sql(f'USE CATALOG {catalog_dev}')\n",
    "spark.sql(f'CREATE SCHEMA IF NOT EXISTS {schema_dev}')\n",
    "spark.sql(f'USE SCHEMA {schema_dev}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dc164bb-5dee-4bbd-ab42-26afb9649edf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def check_for_new_predicted_masks() -> list:\n",
    "    \"\"\"\n",
    "    Function that checks for new predicted mask. Returns a list of new predicted masks.\n",
    "    \"\"\"\n",
    "    all_masks = [f.path for f in dbutils.fs.ls(predicted_masks) if f.name.endswith(\".geojson\")]\n",
    "    processed_masks_df = spark.read.table(log_table).select(\"row_hash\")\n",
    "    processed_masks = [row[\"row_hash\"] for row in processed_masks_df.collect()]\n",
    "\n",
    "    return [mask for mask in all_masks if mask not in processed_masks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47cbe5e9-523a-464c-8da2-2ef1b89e789c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def calculate_diameter(\n",
    "    df: DataFrame,\n",
    "    wkt_col: str = \"geometry\",\n",
    "    id_col: str = \"row_hash\",\n",
    "    out_col: str = \"diameter\"\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Adds a column to df giving the minimum caliper width (shortest \n",
    "    distance across the largest continuous part of each polygon).\n",
    "    \"\"\"\n",
    "    @udf(returnType=DoubleType())\n",
    "    def _min_caliper_width(wkt_str: str) -> float:\n",
    "        geom = wkt.loads(wkt_str)\n",
    "\n",
    "        # Største sammenhengende polygon\n",
    "        if isinstance(geom, MultiPolygon):\n",
    "            geom = sorted(geom.geoms, key=lambda g: g.area)[-1]\n",
    "\n",
    "        # Convex hull for enklere beregninger\n",
    "        hull = geom.convex_hull\n",
    "        coords = list(hull.exterior.coords[:-1])  # Fjerner siste punkt som er likt det første\n",
    "\n",
    "        # Beregner alle perpendikulære avstander fra et punkt til motsatt kant\n",
    "        min_width = float(\"inf\")\n",
    "        for i in range(len(coords)):\n",
    "            a, b = coords[i], coords[(i + 1) % len(coords)]\n",
    "            edge_dx = b[0] - a[0]\n",
    "            edge_dy = b[1] - a[1]\n",
    "            length = (edge_dx**2 + edge_dy**2)**0.5\n",
    "            if length == 0:\n",
    "                continue\n",
    "\n",
    "            # Enhetsvektor perpendikulær til kant\n",
    "            perp_dx, perp_dy = -edge_dy / length, edge_dx / length\n",
    "\n",
    "            # Projiserer alle punkter til den perpendikulære vektoren\n",
    "            projections = [p[0]*perp_dx + p[1]*perp_dy for p in coords]\n",
    "            width = sorted(projections)[-1] - sorted(projections)[0] # Blir det samme som max(projections) - min(projections)\n",
    "            min_width = sorted([min_width, width])[0] # Blir det samme som min(min_width, width)\n",
    "        return float(min_width)\n",
    "    return df.withColumn(out_col, _min_caliper_width(F.col(wkt_col)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9476edc7-5ac7-4cf9-8443-42809ec0a89f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def to_wkt_2d(geom):\n",
    "    if isinstance(geom, BaseGeometry):\n",
    "        return force_2d(geom).wkt\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0835fabb-40ff-498a-97a1-01b100bdb74d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_to_sdf(mask_path: str, mask_name: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Read GeoJSON and write one merged MultiPolygon row to SDF with centroid.\n",
    "    \"\"\"\n",
    "    gdf = gpd.read_file(mask_path)\n",
    "\n",
    "    # Reproject if needed\n",
    "    if gdf.crs != \"EPSG:25833\":\n",
    "        gdf = gdf.set_crs(gdf.crs).to_crs(\"EPSG:25833\")\n",
    "\n",
    "    # Merge all geometries into a single MultiPolygon\n",
    "    merged_geom = unary_union(gdf.geometry)\n",
    "    if merged_geom.geom_type == \"Polygon\":\n",
    "        merged_geom = MultiPolygon([merged_geom])\n",
    "\n",
    "    # Convert to WKT\n",
    "    wkt_geom = to_wkt_2d(merged_geom)\n",
    "\n",
    "    # Calculate centroid\n",
    "    centroid = merged_geom.centroid\n",
    "    centroid_x = centroid.x\n",
    "    centroid_y = centroid.y\n",
    "\n",
    "    # Create single-row DataFrame\n",
    "    df = pd.DataFrame(\n",
    "        [{\"geometry\": wkt_geom, \"centroid_x\": centroid_x, \"centroid_y\": centroid_y}]\n",
    "    )\n",
    "    basic_sdf = spark.createDataFrame(df)\n",
    "\n",
    "    sdf_diameter = calculate_diameter(basic_sdf)\n",
    "    sdf_clean = sdf_diameter.drop(\"geometry\") # Bruker ikke geometry etter dette\n",
    "\n",
    "    # Add metadata\n",
    "    sdf = (\n",
    "        sdf_clean.withColumn(\"ingest_time\", F.current_timestamp())\n",
    "        .withColumn(\"source_file\", F.lit(mask_name))\n",
    "        .withColumn(\"row_hash\", F.sha2(F.concat_ws(\"||\", *sdf_clean.columns), 256))\n",
    "    )\n",
    "\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8db6c44b-4f49-4b7f-9883-53cfce0c6eb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_delta_table(sdf: DataFrame):\n",
    "    \"\"\"\n",
    "    Write delta table from spark dataframe.\n",
    "    \"\"\"\n",
    "    if not spark.catalog.tableExists(table):\n",
    "        sdf.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table)\n",
    "    else:\n",
    "        delta_tbl = DeltaTable.forName(spark, table)\n",
    "        delta_tbl.alias(\"target\").merge(\n",
    "            source=sdf.alias(\"source\"), condition=\"target.row_hash = source.row_hash\"\n",
    "        ).whenMatchedUpdate(\n",
    "            condition=\"target.row_hash != source.row_hash\",\n",
    "            set={col: f\"source.{col}\" for col in sdf.columns},\n",
    "        ).whenNotMatchedInsert(\n",
    "            values={col: f\"source.{col}\" for col in sdf.columns}\n",
    "        ).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5abc345f-0cf2-4d77-990a-f1d1b7efac90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Reads predicted masks and writes them to delta table.\n",
    "    \"\"\"\n",
    "    predicted_masks = check_for_new_predicted_masks()\n",
    "    for mask in predicted_masks:\n",
    "        mask_name = mask.rstrip(\"/\").split(\"/\")[-1]\n",
    "        mask_path = mask.removeprefix(\"dbfs:\")\n",
    "        print(f\"\\nProcessing mask: {mask_name}\")\n",
    "\n",
    "        sdf = write_to_sdf(mask_path, mask_name)\n",
    "        write_delta_table(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbe83d20-7173-4873-8dd9-146265446fbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "predicted_silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
