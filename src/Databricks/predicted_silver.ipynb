{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ecf586b-d682-42a1-897f-70efc5345130",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.ops import unary_union\n",
    "from shapely.geometry import MultiPolygon\n",
    "from shapely.geometry.base import BaseGeometry\n",
    "from shapely import force_2d\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "from delta.tables import DeltaTable\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9c07d4b-3b49-4d39-9044-495fc60de30a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predicted_masks = \"/Volumes/land_topografisk-gdb_dev/external_dev/static_data/DL_SNUPLASSER/predicted_masks\"\n",
    "catalog_dev = \"`land_topografisk-gdb_dev`\"\n",
    "schema_dev = \"ai2025\"\n",
    "log_table = f\"{catalog_dev}.{schema_dev}.logs_predicted_masks\"\n",
    "table = f\"{catalog_dev}.{schema_dev}.predicted_silver\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b983e36-e54a-4007-8da3-a2e5f1af97a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sett kontekst med katalog og skjema \n",
    "spark.sql(f'USE CATALOG {catalog_dev}')\n",
    "spark.sql(f'CREATE SCHEMA IF NOT EXISTS {schema_dev}')\n",
    "spark.sql(f'USE SCHEMA {schema_dev}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dc164bb-5dee-4bbd-ab42-26afb9649edf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def check_for_new_predicted_masks() -> list:\n",
    "    \"\"\"\n",
    "    Function that checks for new predicted mask. Returns a list of new predicted masks.\n",
    "    \"\"\"\n",
    "    all_masks = [f.path for f in dbutils.fs.ls(predicted_masks) if f.name.endswith(\".geojson\")]\n",
    "    processed_masks_df = spark.read.table(log_table).select(\"row_hash\")\n",
    "    processed_masks = [row[\"row_hash\"] for row in processed_masks_df.collect()]\n",
    "\n",
    "    return [mask for mask in all_masks if mask not in processed_masks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9476edc7-5ac7-4cf9-8443-42809ec0a89f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def to_wkt_2d(geom):\n",
    "    if isinstance(geom, BaseGeometry):\n",
    "        return force_2d(geom).wkt\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0835fabb-40ff-498a-97a1-01b100bdb74d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_to_sdf(mask_path: str, mask_name: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Read GeoJSON and write one merged MultiPolygon row to SDF with centroid.\n",
    "    \"\"\"\n",
    "    gdf = gpd.read_file(mask_path)\n",
    "\n",
    "    # Reproject if needed\n",
    "    if gdf.crs != \"EPSG:25833\":\n",
    "        gdf = gdf.set_crs(gdf.crs).to_crs(\"EPSG:25833\")\n",
    "\n",
    "    # Merge all geometries into a single MultiPolygon\n",
    "    merged_geom = unary_union(gdf.geometry)\n",
    "    if merged_geom.geom_type == \"Polygon\":\n",
    "        merged_geom = MultiPolygon([merged_geom])\n",
    "\n",
    "    # Convert to WKT\n",
    "    wkt_geom = to_wkt_2d(merged_geom)\n",
    "\n",
    "    # Calculate centroid\n",
    "    centroid = merged_geom.centroid\n",
    "    centroid_x = centroid.x\n",
    "    centroid_y = centroid.y\n",
    "\n",
    "    # Create single-row DataFrame\n",
    "    df = pd.DataFrame(\n",
    "        [{\"geometry\": wkt_geom, \"centroid_x\": centroid_x, \"centroid_y\": centroid_y}]\n",
    "    )\n",
    "    sdf = spark.createDataFrame(df)\n",
    "\n",
    "    # Add metadata\n",
    "    sdf = (\n",
    "        sdf.withColumn(\"ingest_time\", current_timestamp())\n",
    "        .withColumn(\"source_file\", lit(mask_name))\n",
    "        .withColumn(\"row_hash\", sha2(concat_ws(\"||\", *sdf.columns), 256))\n",
    "    )\n",
    "\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8db6c44b-4f49-4b7f-9883-53cfce0c6eb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_delta_table(sdf: DataFrame):\n",
    "    \"\"\"\n",
    "    Write delta table from spark dataframe.\n",
    "    \"\"\"\n",
    "    if not spark.catalog.tableExists(table):\n",
    "        sdf.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table)\n",
    "    else:\n",
    "        delta_tbl = DeltaTable.forName(spark, table)\n",
    "        delta_tbl.alias(\"target\").merge(\n",
    "            source=sdf.alias(\"source\"), condition=\"target.row_hash = source.row_hash\"\n",
    "        ).whenMatchedUpdate(\n",
    "            condition=\"target.row_hash != source.row_hash\",\n",
    "            set={col: f\"source.{col}\" for col in sdf.columns},\n",
    "        ).whenNotMatchedInsert(\n",
    "            values={col: f\"source.{col}\" for col in sdf.columns}\n",
    "        ).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5abc345f-0cf2-4d77-990a-f1d1b7efac90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Reads predicted masks and writes them to delta table.\n",
    "    \"\"\"\n",
    "    predicted_masks = check_for_new_predicted_masks()\n",
    "    for mask in predicted_masks:\n",
    "        mask_name = mask.rstrip(\"/\").split(\"/\")[-1]\n",
    "        mask_path = mask.removeprefix(\"dbfs:\")\n",
    "        print(f\"\\nProcessing mask: {mask_name}\")\n",
    "\n",
    "        sdf = write_to_sdf(mask_path, mask_name)\n",
    "        write_delta_table(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbe83d20-7173-4873-8dd9-146265446fbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "predicted_silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
