{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f0bc67b-e8a2-4174-9444-1e794cbc7b05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import min as spark_min, max as spark_max\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "from sedona.spark import *\n",
    "\n",
    "import random\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad7ca19e-e775-4f9c-99b7-1359c7c4bad4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# config\n",
    "catalog_dev = \"`land_topografisk-gdb_dev`\"\n",
    "schema_dev = \"ai2025\"\n",
    "log_table = f\"{catalog_dev}.{schema_dev}.logs_processed_gdbs\"\n",
    "bronze_table = f\"{catalog_dev}.{schema_dev}.polygons_bronze\"\n",
    "silver_table = f\"{catalog_dev}.{schema_dev}.polygons_silver\"\n",
    "buffer = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d428224e-0b4c-4101-a816-d43a4cd55485",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_bronze = spark.read.table(bronze_table)\n",
    "df_bronze.display(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af76e18a-046e-46ef-88ed-8f587fb6c6e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_table_to_wkt() -> DataFrame:\n",
    "    \"\"\"\n",
    "    Read a Spark DataFrame column containing WKT and return a Spark DataFrame with the\n",
    "    \"\"\"\n",
    "    df_bronze = spark.read.table(bronze_table).withColumn(\"geometry\", F.expr(\"ST_GeomFromWKT(geometry)\"))\n",
    "    return df_bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6c77d6a-d1ee-4d7a-97ec-ec498b688015",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def make_envelope(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Make a envelope based on the geom.\n",
    "    \"\"\"\n",
    "    return df.withColumn(\"envelope\", F.expr(\"ST_Boundary(geometry)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bf9dcfd-eba3-495f-a807-1a86d108f69d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def random_adjusted_bbox(envelope: list, output_size: int=512) -> list:\n",
    "    \"\"\"\n",
    "    Generate a random bbox based on the envelope.\n",
    "    \"\"\"\n",
    "    xmin, ymin, xmax, ymax = envelope\n",
    "    poly_width = xmax - xmin\n",
    "    poly_height = ymax - ymin\n",
    "\n",
    "    if poly_width > output_size or poly_height > output_size:\n",
    "        return None\n",
    "\n",
    "    max_dx = output_size - poly_width\n",
    "    max_dy = output_size - poly_height\n",
    "\n",
    "    dx = random.uniform(0, max_dx)\n",
    "    dy = random.uniform(0, max_dy)\n",
    "\n",
    "    adjusted_xmin = xmin - dx\n",
    "    adjusted_ymin = ymin - dy\n",
    "    adjusted_xmax = adjusted_xmin + output_size\n",
    "    adjusted_ymax = adjusted_ymin + output_size\n",
    "\n",
    "    return [adjusted_xmin, adjusted_ymin, adjusted_xmax, adjusted_ymax]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6dc4af8-c5bf-49d6-a291-e8acc653f94d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def make_bbox(df: DataFrame, column_name: str) -> DataFrame: \n",
    "    \"\"\"\n",
    "    Make a bounding box from a Spark DataFrame based on the envelope.\n",
    "    \"\"\"\n",
    "    df = df.withColumn(\n",
    "    \"bbox\",\n",
    "    F.expr(f\"\"\"\n",
    "    array(\n",
    "        ST_X(ST_Centroid(envelope)) - (GREATEST(ST_XMax(envelope) - ST_XMin(envelope), ST_YMax(envelope) - ST_YMin(envelope)) / 2 + {buffer}),\n",
    "        ST_Y(ST_Centroid(envelope)) - (GREATEST(ST_XMax(envelope) - ST_XMin(envelope), ST_YMax(envelope) - ST_YMin(envelope)) / 2 + {buffer}),\n",
    "        ST_X(ST_Centroid(envelope)) + (GREATEST(ST_XMax(envelope) - ST_XMin(envelope), ST_YMax(envelope) - ST_YMin(envelope)) / 2 + {buffer}),\n",
    "        ST_Y(ST_Centroid(envelope)) + (GREATEST(ST_XMax(envelope) - ST_XMin(envelope), ST_YMax(envelope) - ST_YMin(envelope)) / 2 + {buffer})\n",
    "    )\n",
    "    \"\"\") # Lager BBOX kolonne som kan brukes videre i WMSene på en enkel måte.\n",
    "    )\n",
    "    df = df.withColumn(\n",
    "        \"Polygons\",\n",
    "        F.expr(\"ST_MakeEnvelope(bbox[0], bbox[1], bbox[2], bbox[3])\")\n",
    "    ) # Lager polygons basert på bboxen \n",
    "    df = df.withColumn(\n",
    "        \"Adjusted_bbox\",\n",
    "        adjusted_bbox_udf(F.col(\"bbox\"))\n",
    "    ).drop(\"envelope\")  # Dropper envelope kolonnen. Denne kolonnen brukes egt bare for å plotte bboxene. Strengt tatt ikke nødvendig å bruke.\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a127e380-443c-4960-bd52-b19484f5bb87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_delta_table(sdf: DataFrame):\n",
    "    \"\"\"\n",
    "    Write delta table from spark dataframe.\n",
    "    \"\"\"\n",
    "    if not spark.catalog.tableExists(silver_table):\n",
    "        sdf.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"overwrite\").saveAsTable(silver_table)\n",
    "    else:\n",
    "        delta_tbl = DeltaTable.forName(spark, silver_table)\n",
    "        delta_tbl.alias(\"target\").merge(\n",
    "                    source=sdf.alias(\"source\"),\n",
    "                    condition=\"target.row_hash = source.row_hash\"\n",
    "                ).whenMatchedUpdate(\n",
    "                    condition=\"target.row_hash != source.row_hash\",\n",
    "                    set={col: f\"source.{col}\" for col in sdf.columns}\n",
    "                ).whenNotMatchedInsert(\n",
    "                    values={col: f\"source.{col}\" for col in sdf.columns}\n",
    "                ).execute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca2520ed-20d7-48f6-80ea-7d2f4807c2ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def plot(df: DataFrame, column_name: str, plot_column: str):\n",
    "    \"\"\"\n",
    "    Plot the bbox.\n",
    "    \"\"\"\n",
    "    bbox_gdf = gpd.GeoDataFrame(\n",
    "    df.toPandas(),\n",
    "    geometry=column_name,\n",
    "    crs=\"EPSG:25833\",  # Bruker ETRS89 / UTM sone 33N (Norge)\n",
    "    )\n",
    "    return bbox_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c927767-2831-4a12-bd25-1a8867ce8d9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "adjusted_bbox_udf = F.udf(lambda envelope: random_adjusted_bbox(envelope), ArrayType(DoubleType()))\n",
    "\n",
    "df = read_table_to_wkt()\n",
    "df = make_envelope(df)\n",
    "df = make_bbox(df, \"envelope\")\n",
    "gdf = plot(df, \"Polygons\", \"Vegtyper\")\n",
    "write_delta_table(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0496dc74-4d65-4c93-98ee-701414a8cb4e",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1752239048766}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5724bec4-25f2-4cdc-aa9e-6991bc8f2901",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment under hvis man ønsker å se på plotten\n",
    "# gdf.explore(column=\"Vegtyper\", tooltip=\"Vegtyper\", popup=True, cmap=\"Set1\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5031113679203828,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "polygons_silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
