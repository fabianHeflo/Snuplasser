{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c40d2faf-3d30-44ac-846b-d5bbf7bb6691",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from delta.tables import DeltaTable\n",
    "import requests\n",
    "import time\n",
    "from pathlib import Path\n",
    "import os\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eae7089e-f8a0-44d1-8ef3-69724537f6b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "catalog_dev = \"`land_topografisk-gdb_dev`\"\n",
    "schema_dev = \"ai2025\"\n",
    "spark.sql(f\"USE CATALOG {catalog_dev}\")\n",
    "spark.sql(f\"USE SCHEMA {schema_dev}\")\n",
    "\n",
    "bronze_table = f\"{catalog_dev}.{schema_dev}.endepunkt_bronze\"\n",
    "silver_table = f\"{catalog_dev}.{schema_dev}.endepunkt_silver\"\n",
    "log_table = f\"{catalog_dev}.{schema_dev}.logs_processed_endepunkter\"\n",
    "buffer = 20\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"silver_pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0092be6-34d5-4564-9afe-bde89faed111",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "q = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {log_table} (\n",
    "    kommune_id STRING,\n",
    "    processed_time TIMESTAMP,\n",
    "    num_inserted INT,\n",
    "    num_updated INT,\n",
    "    num_deleted INT\n",
    ") USING DELTA\n",
    "\"\"\"\n",
    "spark.sql(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "943ca59a-f153-40a3-8f66-6c35a44e40b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def log_endepunkt_run(kommune_id: str, inserted: int, updated: int, deleted: int = 0):\n",
    "    schema = StructType([\n",
    "        StructField(\"kommune_id\", StringType(), True),\n",
    "        StructField(\"inserted\", IntegerType(), True),\n",
    "        StructField(\"updated\", IntegerType(), True),\n",
    "        StructField(\"deleted\", IntegerType(), True),\n",
    "    ])\n",
    "    \n",
    "    data = [(kommune_id, inserted, updated, deleted)]\n",
    "    \n",
    "    df_log = spark.createDataFrame(data, schema=schema)\n",
    "    \n",
    "    (df_log.write\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .mode(\"append\")\n",
    "        .saveAsTable(\"your_log_table_name\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6ee2a6a-1f78-4940-bf5c-a85829ef4794",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def add_silver_columns(df: DataFrame, buffer: int = 50, kommune_id: str = \"\") -> DataFrame:\n",
    "    df = df.withColumn(\"bbox\", expr(f\"array(x - {buffer}, y - {buffer}, x + {buffer}, y + {buffer})\"))\n",
    "    df = df.withColumn(\"image_path\", lit(None).cast(\"string\")) \\\n",
    "           .withColumn(\"dom_path\", lit(None).cast(\"string\")) \\\n",
    "           .withColumn(\"image_status\", lit(\"PENDING\")) \\\n",
    "           .withColumn(\"dom_status\", lit(\"PENDING\")) \\\n",
    "           .withColumn(\"lastet_tid\", current_timestamp()) \\\n",
    "           .withColumn(\"kommune_id\", lit(kommune_id)) \\\n",
    "           .withColumn(\"row_hash\", sha2(concat_ws(\"||\", *df.columns), 256))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a60934ed-4c02-4729-8606-173646773b6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_delta_table(sdf: DataFrame, mode: str = \"merge\") -> None:\n",
    "    if mode == \"overwrite\":\n",
    "        sdf.write.format(\"delta\") \\\n",
    "            .option(\"mergeSchema\", \"true\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .saveAsTable(silver_table)\n",
    "    else:\n",
    "        from delta.tables import DeltaTable\n",
    "\n",
    "        delta_tbl = DeltaTable.forName(spark, silver_table)\n",
    "\n",
    "        delta_tbl.alias(\"target\").merge(\n",
    "            sdf.alias(\"source\"),\n",
    "            condition=\"target.row_hash = source.row_hash\" \n",
    "        ).whenMatchedUpdate(\n",
    "            condition=\"target.hentet_tid < source.hentet_tid\", \n",
    "            set={col: f\"source.{col}\" for col in sdf.columns}\n",
    "        ).whenNotMatchedInsert(\n",
    "            values={col: f\"source.{col}\" for col in sdf.columns}\n",
    "        ).execute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d13479b8-10b3-4813-926f-c7e0382050de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_silver_for_kommune(kommune_id: str) -> None:\n",
    "    kommune_id = str(kommune_id)  # sikkerhet\n",
    "    bronze_df = spark.read.table(bronze_table)\n",
    "    bronze_df = bronze_df.filter(col(\"kommune_id\") == lit(kommune_id))\n",
    "\n",
    "    silver_df = add_silver_columns(bronze_df, buffer=buffer)\n",
    "\n",
    "    if not spark.catalog.tableExists(silver_table):\n",
    "        write_delta_table(silver_df, mode=\"overwrite\")\n",
    "    else:\n",
    "        expected_schema = spark.table(silver_table).schema\n",
    "\n",
    "        silver_df = silver_df.select([\n",
    "            lit(\"\").cast(\"string\").alias(c.name) if c.dataType.typeName() == \"void\"\n",
    "            else col(c.name).cast(c.dataType)\n",
    "            for c in expected_schema\n",
    "        ])\n",
    "\n",
    "        write_delta_table(silver_df)\n",
    "\n",
    "    log_endepunkt_run(kommune_id, inserted=silver_df.count(), updated=0)\n",
    "    logger.info(f\"âœ… Endepunkt silver opprettet eller oppdatert med {silver_df.count()} rader.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "855fc0c6-e6c2-4965-b1e3-03cad9032473",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "kommune_id_rows = [\n",
    "    row.asDict() for row in spark.read.table(bronze_table).select(\"kommune_id\").distinct().collect()\n",
    "]\n",
    "\n",
    "\n",
    "for row in kommune_id_rows:\n",
    "    print(f\"Row: {row}, type: {type(row)}, kommune_id: {row['kommune_id']}, type: {type(row['kommune_id'])}\")\n",
    "\n",
    "    kommune_id = row[\"kommune_id\"]\n",
    "    if kommune_id is not None:\n",
    "        kommune_id = str(kommune_id)\n",
    "        process_silver_for_kommune(kommune_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08220333-f645-4764-af1b-a5b3fb708a5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# spark.sql(f\"DROP TABLE IF EXISTS {silver_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {log_table}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "endepunkt_silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
