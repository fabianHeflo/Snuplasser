{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1481dc74-3973-44cd-aa91-d85da73bb491",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if spark.conf.get(\"spark.databricks.clusterUsageTags.clusterUnityCatalogMode\") != \"USER_ISOLATION\":\n",
    "    print(\"Dette er ikke et felles cluster!\")\n",
    "else:\n",
    "    print(\"Kjøres på korrekt jobbkluster med single-user mode.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5df41a4-6611-4f48-84d8-bec74baf9a53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67943897-fa8e-4f67-a06a-28ce56f617fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd95831c-e590-47ac-ba8e-c1b3a2248cac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)\n",
    "print(torch.backends.cuda.is_built())\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb0dcca2-6df2-4b48-9058-b78d7e8cd65b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#%run \"./dataProcessing/transform\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c519bfc0-b61c-41fb-bfa0-3b67d55fd2c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "import sys\n",
    "import os\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "\n",
    "from dataProcessing.dataset import SnuplassDataset, load_numpy_split_stack\n",
    "from model.unet import UNet\n",
    "from dataProcessing.augmentation_config import augmentation_profiles\n",
    "#from utils import iou_pytorch, acc_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae238a72-eaf8-40ec-b91e-b66af840161c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    mlflow.pytorch.autolog()  # Lagrer modellen under Experiments. Kan hente modellen med model = mlflow.pytorch.load_model(\"runs://model\")\n",
    "\n",
    "    cfg = augmentation_profiles[\"default\"]\n",
    "    batch_size = 8\n",
    "    num_epochs = 1\n",
    "    learning_rate = 1e-3\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    #log_dir = \"runs/snuplasser\"\n",
    "    #writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    train_ids, val_ids, _ = load_numpy_split_stack(\n",
    "        image_dir=\"/Volumes/land_topografisk-gdb_dev/external_dev/static_data/DL_SNUPLASSER/img/\",\n",
    "        mask_dir=\"/Volumes/land_topografisk-gdb_dev/external_dev/static_data/DL_SNUPLASSER/lab/\",\n",
    "        dom_dir=\"/Volumes/land_topografisk-gdb_dev/external_dev/static_data/DL_SNUPLASSER/dom/\",\n",
    "    )\n",
    "\n",
    "    train_dataset = SnuplassDataset(\n",
    "        image_dir=\"/Volumes/land_topografisk-gdb_dev/external_dev/static_data/DL_SNUPLASSER/img/\",\n",
    "        mask_dir=\"/Volumes/land_topografisk-gdb_dev/external_dev/static_data/DL_SNUPLASSER/lab/\",\n",
    "        dom_dir=\"/Volumes/land_topografisk-gdb_dev/external_dev/static_data/DL_SNUPLASSER/dom/\",\n",
    "        file_list=train_ids,\n",
    "        transform=None,  # ratio=None for baseline\n",
    "        # For å bruke augmentering, sett ratio til en verdi mellom 0 og 1\n",
    "    )\n",
    "\n",
    "    val_dataset = SnuplassDataset(\n",
    "        image_dir=\"/Volumes/land_topografisk-gdb_dev/external_dev/static_data/DL_SNUPLASSER/img/\",\n",
    "        mask_dir=\"/Volumes/land_topografisk-gdb_dev/external_dev/static_data/DL_SNUPLASSER/lab/\",\n",
    "        dom_dir=\"/Volumes/land_topografisk-gdb_dev/external_dev/static_data/DL_SNUPLASSER/dom/\",\n",
    "        file_list=val_ids,\n",
    "        transform=None,\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = UNet(n_channels=4, n_classes=1, bilinear=False).to(\n",
    "        device\n",
    "    )  # bare å bytte modell\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    with mlflow.start_run(run_name=\"UNet_baseline_4ch\"):\n",
    "        for epoch in range(num_epochs):\n",
    "            # Trening\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "\n",
    "            for images, masks in tqdm(\n",
    "                train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"\n",
    "            ):\n",
    "                images, masks = images.to(device).float(), masks.to(device).float()\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs.squeeze(1), masks)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            avg_train_loss = total_loss / len(train_loader)\n",
    "            print(f\"\\nTrain loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "            #writer.add_scalar(\"Tap/Trening\", avg_train_loss, epoch)\n",
    "\n",
    "            # Validering\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            #val_ious = []\n",
    "            #val_accs = []\n",
    "            with torch.no_grad():\n",
    "                for images, masks in tqdm(\n",
    "                    val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"\n",
    "                ):\n",
    "                    images, masks = images.to(device).float(), masks.to(device).float()\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs.squeeze(1), masks)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "                    # Beregn IoU og accuracy\n",
    "                    #predictions = (\n",
    "                    #    torch.sigmoid(outputs) > 0.5\n",
    "                    #).int()  # Konverterer til binære prediksjoner\n",
    "                    #iou = iou_pytorch(predictions, masks.int())\n",
    "                    #acc = acc_pytorch(predictions, masks.int())\n",
    "                    #val_ious.append(iou.item())\n",
    "                    #val_accs.append(acc.item())\n",
    "\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            print(f\"Val loss: {avg_val_loss:.4f}\")\n",
    "            #avg_iou = sum(val_ious) / len(val_ious)\n",
    "            #avg_acc = sum(val_accs) / len(val_accs)\n",
    "\n",
    "            #writer.add_scalar(\"Tap/Validering\", avg_val_loss, epoch)\n",
    "            #writer.add_scalar(\"Metrikker/IoU\", avg_iou, epoch)\n",
    "            #writer.add_scalar(\"Metrikker/Accuracy\", avg_acc, epoch)\n",
    "\n",
    "        #writer.close()\n",
    "\n",
    "    print(\"✅ Trening ferdig\")\n",
    "\n",
    "main()\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "train",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
